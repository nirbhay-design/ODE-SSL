environment: 
YAML: configs/lema.c10.yaml
==> SEED: 42
==> num_classes: 10
==> data_name: cifar10
==> data_params: {'algo': 'lema', 'data_dir': 'datasets/cifar10', 'batch_size': 512, 'num_workers': 3, 'image_size': 32}
==> return_logs: False
==> eval_every: 10
==> n_epochs: 800
==> n_epochs_mlp: 100
==> gpu_id: 0
==> opt: SGD
==> opt_params: {'lr': 0.5, 'momentum': 0.9, 'nesterov': True}
==> schedular_params: {'T_max': 800, 'eta_min': 0.001}
==> mlp_opt: SGD
==> mlp_opt_params: {'lr': 0.01, 'momentum': 0.9, 'nesterov': True}
==> energy_model_params: {'eta': 0.01, 'steps': 5}
==> energy_opt: AdamW
==> energy_model_opt_params: {'lr': 0.001}
==> model_params: {'model_name': 'resnet18', 'pretrained': False, 'proj_dim': 128, 'algo_type': 'lema'}
==> mlp_type: linear
==> loss: lema
==> loss_params: {'sim': 'cosine', 'tau': 0.5}
==> distributed: False
==> train_algo: lema
==> model_save_path: saved_models/lema.c10.r18.v3.e800.lin.pth
==> config: configs/lema.c10.yaml
--------------------------------------------------
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
# of Training Images: 50000
# of Testing Images: 10000
===> using linear mlp
using optimizer: SGD
using optimizer: SGD
loss function: lema
using optimizer: AdamW
### LEMa Training begins
[GPU0] epochs: [1/800] train_loss_con: 6.375 energy_loss: -0.276
[GPU0] epochs: [2/800] train_loss_con: 6.148 energy_loss: -0.211
[GPU0] epochs: [3/800] train_loss_con: 6.061 energy_loss: -0.033
[GPU0] epochs: [4/800] train_loss_con: 5.984 energy_loss: -0.362
[GPU0] epochs: [5/800] train_loss_con: 5.929 energy_loss: -0.139
[GPU0] epochs: [6/800] train_loss_con: 5.891 energy_loss: -0.285
